[stdout]: Initializing model...
[stdout]: Tried to build Llama model with cfg.name=mosaic_llama2_1.3b
[stdout]: ComposerMosaicLlama(
[stdout]:   (model): LlamaModel(
[stdout]:     (transformer): ModuleDict(
[stdout]:       (wte): Embedding(32000, 2048)
[stdout]:       (blocks): ModuleList(
[stdout]:         (0-23): 24 x LlamaBlock(
[stdout]:           (ln_1): LlamaRMSNorm()
[stdout]:           (attn): LlamaAttention(
[stdout]:             (wq): Linear(in_features=2048, out_features=2048, bias=False)
[stdout]:             (wk): Linear(in_features=2048, out_features=2048, bias=False)
[stdout]:             (wv): Linear(in_features=2048, out_features=2048, bias=False)
[stdout]:             (out_proj): Linear(in_features=2048, out_features=2048, bias=False)
[stdout]:             (rotary_emb): LlamaRotaryEmbedding()
[stdout]:           )
[stdout]:           (ln_2): LlamaRMSNorm()
[stdout]:           (mlp): LlamaMLP(
[stdout]:             (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
[stdout]:             (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
[stdout]:             (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
[stdout]:           )
[stdout]:         )
[stdout]:       )
[stdout]:       (ln_f): LlamaRMSNorm()
[stdout]:       (output): Linear(in_features=2048, out_features=32000, bias=False)
[stdout]:     )
[stdout]:   )
[stdout]: )
[stdout]: None
[stdout]: Loaded model from path:  /home/shuyaoli/LLM-Shearing/models/LLaMA-1-3-B-Pruned/state_dict.pt
[stdout]: Model load state dict result:  _IncompatibleKeys(missing_keys=['model.transformer.blocks.0.attn.rotary_emb.inv_freq', 'model.transformer.blocks.1.attn.rotary_emb.inv_freq', 'model.transformer.blocks.2.attn.rotary_emb.inv_freq', 'model.transformer.blocks.3.attn.rotary_emb.inv_freq', 'model.transformer.blocks.4.attn.rotary_emb.inv_freq', 'model.transformer.blocks.5.attn.rotary_emb.inv_freq', 'model.transformer.blocks.6.attn.rotary_emb.inv_freq', 'model.transformer.blocks.7.attn.rotary_emb.inv_freq', 'model.transformer.blocks.8.attn.rotary_emb.inv_freq', 'model.transformer.blocks.9.attn.rotary_emb.inv_freq', 'model.transformer.blocks.10.attn.rotary_emb.inv_freq', 'model.transformer.blocks.11.attn.rotary_emb.inv_freq', 'model.transformer.blocks.12.attn.rotary_emb.inv_freq', 'model.transformer.blocks.13.attn.rotary_emb.inv_freq', 'model.transformer.blocks.14.attn.rotary_emb.inv_freq', 'model.transformer.blocks.15.attn.rotary_emb.inv_freq', 'model.transformer.blocks.16.attn.rotary_emb.inv_freq', 'model.transformer.blocks.17.attn.rotary_emb.inv_freq', 'model.transformer.blocks.18.attn.rotary_emb.inv_freq', 'model.transformer.blocks.19.attn.rotary_emb.inv_freq', 'model.transformer.blocks.20.attn.rotary_emb.inv_freq', 'model.transformer.blocks.21.attn.rotary_emb.inv_freq', 'model.transformer.blocks.22.attn.rotary_emb.inv_freq', 'model.transformer.blocks.23.attn.rotary_emb.inv_freq'], unexpected_keys=[])
[stdout]: Having missing rotary_emb.inv_freq keys is normal
[stdout]: cfg.n_params=1.35e+09
[stdout]: model.num_fwd_flops=1.43e+13
[stdout]: Building train loader...
[stderr]: /opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[stderr]:   warnings.warn(
[stdout]: Building eval loader...
[stdout]: Group 0: 219 tensors 1345423360 params 1.00e-04 lr
[stdout]: Target loss: [1.9643, 0.7459, 2.1393, 1.6117, 1.759, 1.4449, 2.1251]
[stdout]: Building trainer...
[stderr]: [34m[1mwandb[0m: Tracking run with wandb version 0.15.12
[stderr]: [34m[1mwandb[0m: W&B syncing is set to [1m`offline`[0m in this directory.  
[stderr]: [34m[1mwandb[0m: Run [1m`wandb online`[0m or set [1mWANDB_MODE=online[0m to enable cloud syncing.
[stderr]: [34m[1mwandb[0m: [33mWARNING[0m URL not available in offline run
[stderr]: /opt/conda/lib/python3.10/site-packages/composer/callbacks/memory_monitor.py:94: UserWarning: The memory monitor only works on CUDA devices, but the model is on cpu.
[stderr]:   warnings.warn(f'The memory monitor only works on CUDA devices, but the model is on {model_device.type}.')
[trace]: algorithm_traces/GradientClipping/Event.INIT:1
[hyperparameter]: num_nodes: 1 
[hyperparameter]: num_gpus_per_node: 1 
[hyperparameter]: node_name: "unknown because NODENAME environment variable not set" 
[hyperparameter]: rank_zero_seed: 17 
[stdout]: Logging config...
[stdout]: data_local: /home/shuyaoli/LLM-Shearing/llm_dataset/LLM-Shearing/for_prune
[stdout]: data_remote: null
[stdout]: tokenizer_name: meta-llama/Llama-2-7b-hf
[stdout]: max_seq_len: 4096
[stdout]: global_seed: 17
[stdout]: run_name: LLaMA-1-3-B-Pruned_ft800ba_Adam
[stdout]: model:
[stdout]:   name: mosaic_llama2_1.3b
[stdout]:   init_device: cpu
[stdout]:   tokenizer_name: ${tokenizer_name}
[stdout]:   d_model: 2048
[stdout]:   n_heads: 16
[stdout]:   n_layers: 24
[stdout]:   intermediate_size: 5504
[stdout]:   max_seq_len: ${max_seq_len}
[stdout]:   vocab_size: 32000
[stdout]:   init_std: 0.02
[stdout]:   attn_pdrop: 0.0
[stdout]:   resid_pdrop: 0.0
[stdout]:   emb_pdrop: 0.0
[stdout]:   attn_impl: flash
[stdout]:   rms_norm_eps: 1.0e-05
[stdout]:   l0_module: null
[stdout]:   path: /home/shuyaoli/LLM-Shearing/models/LLaMA-1-3-B-Pruned/state_dict.pt
[stdout]:   set_names:
[stdout]:   - cc
[stdout]:   - github
[stdout]:   - book
[stdout]:   - stackexchange
[stdout]:   - wiki
[stdout]:   - arxiv
[stdout]:   - c4-rp
[stdout]: tokenizer:
[stdout]:   type: hftokenizer
[stdout]:   args:
[stdout]:     tokenizer_name: ${tokenizer_name}
[stdout]:     max_seq_len: ${max_seq_len}
[stdout]: train_loader:
[stdout]:   name: text
[stdout]:   dataset:
[stdout]:     local: ${data_local}
[stdout]:     remote: ${data_remote}
[stdout]:     split: train_small
[stdout]:     shuffle: true
[stdout]:     tokenizer_name: ${tokenizer_name}
[stdout]:     max_seq_len: ${max_seq_len}
[stdout]:     shuffle_seed: ${global_seed}
[stdout]:     is_uint16: true
[stdout]:   drop_last: true
[stdout]:   num_workers: 0
[stdout]:   prefetch_factor: null
[stdout]:   persistent_workers: false
[stdout]: eval_loader:
[stdout]:   name: text
[stdout]:   dataset:
[stdout]:     local: ${data_local}
[stdout]:     remote: ${data_remote}
[stdout]:     split: eval_merge
[stdout]:     shuffle: false
[stdout]:     tokenizer_name: ${tokenizer_name}
[stdout]:     max_seq_len: ${max_seq_len}
[stdout]:     shuffle_seed: ${global_seed}
[stdout]:     is_uint16: true
[stdout]:   drop_last: false
[stdout]:   num_workers: 8
[stdout]: scheduler:
[stdout]:   t_warmup: 24ba
[stdout]:   alpha_f: 0.1
[stdout]: optimizer:
[stdout]:   lr: 0.0001
[stdout]:   betas:
[stdout]:   - 0.9
[stdout]:   - 0.95
[stdout]:   eps: 1.0e-08
[stdout]:   weight_decay: 0.0
[stdout]: algorithms:
[stdout]:   gradient_clipping:
[stdout]:     clipping_type: norm
[stdout]:     clipping_threshold: 1.0
[stdout]: max_duration: 800ba
[stdout]: eval_interval: 5ba
[stdout]: eval_subset_num_batches: 1000
[stdout]: global_train_batch_size: 256
[stdout]: seed: ${global_seed}
[stdout]: device_eval_batch_size: 8
[stdout]: device_train_microbatch_size: 16
[stdout]: precision: amp_bf16
[stdout]: fsdp_config:
[stdout]:   sharding_strategy: FULL_SHARD
[stdout]:   mixed_precision: DEFAULT
[stdout]:   activation_checkpointing: true
[stdout]:   activation_cpu_offload: false
[stdout]:   verbose: false
[stdout]: progress_bar: false
[stdout]: log_to_console: true
[stdout]: console_log_interval: 1ba
[stdout]: callbacks:
[stdout]:   speed_monitor:
[stdout]:     window_size: 10
[stdout]:   memory_monitor: {}
[stdout]:   lr_monitor: {}
[stdout]: loggers:
[stdout]:   wandb:
[stdout]:     project: pruning
[stdout]:     entity: pruning
[stdout]:     name: ${run_name}
[stdout]:     init_kwargs:
[stdout]:       mode: offline
[stdout]:       dir: /home/shuyaoli/LLM-Shearing/models/pretrained/LLaMA-1-3-B-Pruned_ft800ba_Adam
[stdout]:       project: pruning
[stdout]:       name: LLaMA-1-3-B-Pruned_ft800ba_Adam
[stdout]:       entity: pruning
[stdout]: save_interval: 10ba
[stdout]: save_folder: /home/shuyaoli/LLM-Shearing/models/pretrained/LLaMA-1-3-B-Pruned_ft800ba_Adam
[stdout]: eval_first: false
[stdout]: autoresume: false
[stdout]: dist_timeout: 1800.0
[stdout]: n_gpus: 1
[stdout]: device_train_batch_size: 256
[stdout]: device_train_grad_accum: 16
[stdout]: n_params: 1345423360

[stdout]: Starting training...
[hyperparameter]: enabled_algorithms/GradientClipping: True 
[stderr]: ******************************
[stderr]: Config:
[stderr]: enabled_algorithms/GradientClipping: true
[stderr]: node_name: unknown because NODENAME environment variable not set
[stderr]: num_gpus_per_node: 1
[stderr]: num_nodes: 1
[stderr]: rank_zero_seed: 17

[stderr]: ******************************
[metric][batch=0]: time/epoch: 0 
[metric][batch=0]: time/batch: 0 
[metric][batch=0]: time/sample: 0 
[metric][batch=0]: time/batch_in_epoch: 0 
[metric][batch=0]: time/sample_in_epoch: 0 
[metric][batch=0]: time/token: 0 
[metric][batch=0]: time/token_in_epoch: 0 
[trace]: algorithm_traces/GradientClipping/Event.AFTER_TRAIN_BATCH:1
[metric][batch=0]: metrics/train/cc_weight: 0.2192 
[metric][batch=0]: metrics/train/github_weight: 0.0002 
[metric][batch=0]: metrics/train/book_weight: 0.0791 
[metric][batch=0]: metrics/train/stackexchange_weight: 0.0064 
[metric][batch=0]: metrics/train/wiki_weight: 0.0096 
[metric][batch=0]: metrics/train/arxiv_weight: 0.0010 
[metric][batch=0]: metrics/train/c4-rp_weight: 0.6845 
[metric][batch=0]: memory/current_allocated_mem: 15.0910 
[metric][batch=0]: memory/current_active_mem: 15.0910 
[metric][batch=0]: memory/current_inactive_mem: 0.0248 
[metric][batch=0]: memory/current_reserved_mem: 51.7280 
[metric][batch=0]: memory/peak_allocated_mem: 44.9390 
[metric][batch=0]: memory/peak_active_mem: 45.3020 
[metric][batch=0]: memory/peak_inactive_mem: 15.8900 
[metric][batch=0]: memory/peak_reserved_mem: 51.7280 
[metric][batch=0]: memory/alloc_retries: 0 
[metric][batch=0]: trainer/device_train_microbatch_size: 16 
[metric][batch=0]: loss/train/total: 2.7483 
[metric][batch=0]: loss/train/ce_loss: 2.7483 
[metric][batch=0]: metrics/train/LanguageCrossEntropy: 2.7483 
[metric][batch=0]: metrics/train/Perplexity: 15.6166 
[metric][batch=0]: metrics/train/cc_LanguageCrossEntropy: 2.5488 
[metric][batch=0]: metrics/train/cc_count: 56 
[metric][batch=0]: metrics/train/github_LanguageCrossEntropy: nan 
[metric][batch=0]: metrics/train/github_count: 0 
[metric][batch=0]: metrics/train/book_LanguageCrossEntropy: 2.9071 
[metric][batch=0]: metrics/train/book_count: 16 
[metric][batch=0]: metrics/train/stackexchange_LanguageCrossEntropy: nan 
[metric][batch=0]: metrics/train/stackexchange_count: 0 
[metric][batch=0]: metrics/train/wiki_LanguageCrossEntropy: 2.1591 
[metric][batch=0]: metrics/train/wiki_count: 4 
[metric][batch=0]: metrics/train/arxiv_LanguageCrossEntropy: nan 
[metric][batch=0]: metrics/train/arxiv_count: 0 
[metric][batch=0]: metrics/train/c4-rp_LanguageCrossEntropy: 2.8094 
[metric][batch=0]: metrics/train/c4-rp_count: 180 
[metric][batch=1]: time/train: 0.0291 
[metric][batch=1]: time/val: 0.0000 
[metric][batch=1]: time/total: 0.0291 
[metric][batch=1]: lr-DecoupledAdamW/group0: 0.0000 
[stderr]: [batch=1/800]:
[stderr]: 	 Train time/epoch: 0
[stderr]: 	 Train time/batch: 0
[stderr]: 	 Train time/sample: 0
[stderr]: 	 Train time/batch_in_epoch: 0
[stderr]: 	 Train time/sample_in_epoch: 0
[stderr]: 	 Train time/token: 0
[stderr]: 	 Train time/token_in_epoch: 0
[stderr]: 	 Train metrics/train/cc_weight: 0.2192
[stderr]: 	 Train metrics/train/github_weight: 0.0002
[stderr]: 	 Train metrics/train/book_weight: 0.0791
[stderr]: 	 Train metrics/train/stackexchange_weight: 0.0064
[stderr]: 	 Train metrics/train/wiki_weight: 0.0096
[stderr]: 	 Train metrics/train/arxiv_weight: 0.0010
[stderr]: 	 Train metrics/train/c4-rp_weight: 0.6845
[stderr]: 	 Train memory/current_allocated_mem: 15.0910
[stderr]: 	 Train memory/current_active_mem: 15.0910
[stderr]: 	 Train memory/current_inactive_mem: 0.0248
[stderr]: 	 Train memory/current_reserved_mem: 51.7280
[stderr]: 	 Train memory/peak_allocated_mem: 44.9390
[stderr]: 	 Train memory/peak_active_mem: 45.3020
[stderr]: 	 Train memory/peak_inactive_mem: 15.8900
[stderr]: 	 Train memory/peak_reserved_mem: 51.7280
[stderr]: 	 Train memory/alloc_retries: 0
[stderr]: 	 Train trainer/device_train_microbatch_size: 16
[stderr]: 	 Train loss/train/total: 2.7483
[stderr]: 	 Train loss/train/ce_loss: 2.7483
[stderr]: 	 Train metrics/train/LanguageCrossEntropy: 2.7483
[stderr]: 	 Train metrics/train/Perplexity: 15.6166
[stderr]: 	 Train metrics/train/cc_LanguageCrossEntropy: 2.5488
[stderr]: 	 Train metrics/train/cc_count: 56
[stderr]: 	 Train metrics/train/github_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/github_count: 0
[stderr]: 	 Train metrics/train/book_LanguageCrossEntropy: 2.9071
[stderr]: 	 Train metrics/train/book_count: 16
[stderr]: 	 Train metrics/train/stackexchange_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/stackexchange_count: 0
[stderr]: 	 Train metrics/train/wiki_LanguageCrossEntropy: 2.1591
[stderr]: 	 Train metrics/train/wiki_count: 4
[stderr]: 	 Train metrics/train/arxiv_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/arxiv_count: 0
[stderr]: 	 Train metrics/train/c4-rp_LanguageCrossEntropy: 2.8094
[stderr]: 	 Train metrics/train/c4-rp_count: 180
[stderr]: 	 Train time/train: 0.0291
[stderr]: 	 Train time/val: 0.0000
[stderr]: 	 Train time/total: 0.0291
[stderr]: 	 Train lr-DecoupledAdamW/group0: 0.0000
[metric][batch=1]: time/batch: 1 
[metric][batch=1]: time/sample: 256 
[metric][batch=1]: time/batch_in_epoch: 1 
[metric][batch=1]: time/sample_in_epoch: 256 
[metric][batch=1]: time/token: 1048576 
[metric][batch=1]: time/token_in_epoch: 1048576 
[trace]: algorithm_traces/GradientClipping/Event.AFTER_TRAIN_BATCH:1
[metric][batch=1]: metrics/train/cc_weight: 0.2192 
[metric][batch=1]: metrics/train/github_weight: 0.0002 
[metric][batch=1]: metrics/train/book_weight: 0.0791 
[metric][batch=1]: metrics/train/stackexchange_weight: 0.0064 
[metric][batch=1]: metrics/train/wiki_weight: 0.0096 
[metric][batch=1]: metrics/train/arxiv_weight: 0.0010 
[metric][batch=1]: metrics/train/c4-rp_weight: 0.6845 
[metric][batch=1]: memory/current_allocated_mem: 25.8550 
[metric][batch=1]: memory/current_active_mem: 25.8550 
[metric][batch=1]: memory/current_inactive_mem: 0.5230 
[metric][batch=1]: memory/current_reserved_mem: 60.1190 
[metric][batch=1]: memory/peak_allocated_mem: 55.7030 
[metric][batch=1]: memory/peak_active_mem: 56.0670 
[metric][batch=1]: memory/peak_inactive_mem: 17.5110 
[metric][batch=1]: memory/peak_reserved_mem: 60.1190 
[metric][batch=1]: memory/alloc_retries: 0 
[metric][batch=1]: trainer/device_train_microbatch_size: 16 
[metric][batch=1]: loss/train/total: 2.7294 
[metric][batch=1]: loss/train/ce_loss: 2.7294 
[metric][batch=1]: metrics/train/LanguageCrossEntropy: 2.7294 
[metric][batch=1]: metrics/train/Perplexity: 15.3243 
[metric][batch=1]: metrics/train/cc_LanguageCrossEntropy: 2.5883 
[metric][batch=1]: metrics/train/cc_count: 111 
[metric][batch=1]: metrics/train/github_LanguageCrossEntropy: nan 
[metric][batch=1]: metrics/train/github_count: 0 
[metric][batch=1]: metrics/train/book_LanguageCrossEntropy: 2.7637 
[metric][batch=1]: metrics/train/book_count: 36 
[metric][batch=1]: metrics/train/stackexchange_LanguageCrossEntropy: 2.1109 
[metric][batch=1]: metrics/train/stackexchange_count: 3 
[metric][batch=1]: metrics/train/wiki_LanguageCrossEntropy: 2.2021 
[metric][batch=1]: metrics/train/wiki_count: 8 
[metric][batch=1]: metrics/train/arxiv_LanguageCrossEntropy: nan 
[metric][batch=1]: metrics/train/arxiv_count: 0 
[metric][batch=1]: metrics/train/c4-rp_LanguageCrossEntropy: 2.7929 
[metric][batch=1]: metrics/train/c4-rp_count: 354 
[metric][batch=2]: time/train: 0.0580 
[metric][batch=2]: time/val: 0.0000 
[metric][batch=2]: time/total: 0.0580 
[metric][batch=2]: lr-DecoupledAdamW/group0: 0.0000 
[stderr]: [batch=2/800]:
[stderr]: 	 Train time/batch: 1
[stderr]: 	 Train time/sample: 256
[stderr]: 	 Train time/batch_in_epoch: 1
[stderr]: 	 Train time/sample_in_epoch: 256
[stderr]: 	 Train time/token: 1048576
[stderr]: 	 Train time/token_in_epoch: 1048576
[stderr]: 	 Train metrics/train/cc_weight: 0.2192
[stderr]: 	 Train metrics/train/github_weight: 0.0002
[stderr]: 	 Train metrics/train/book_weight: 0.0791
[stderr]: 	 Train metrics/train/stackexchange_weight: 0.0064
[stderr]: 	 Train metrics/train/wiki_weight: 0.0096
[stderr]: 	 Train metrics/train/arxiv_weight: 0.0010
[stderr]: 	 Train metrics/train/c4-rp_weight: 0.6845
[stderr]: 	 Train memory/current_allocated_mem: 25.8550
[stderr]: 	 Train memory/current_active_mem: 25.8550
[stderr]: 	 Train memory/current_inactive_mem: 0.5230
[stderr]: 	 Train memory/current_reserved_mem: 60.1190
[stderr]: 	 Train memory/peak_allocated_mem: 55.7030
[stderr]: 	 Train memory/peak_active_mem: 56.0670
[stderr]: 	 Train memory/peak_inactive_mem: 17.5110
[stderr]: 	 Train memory/peak_reserved_mem: 60.1190
[stderr]: 	 Train memory/alloc_retries: 0
[stderr]: 	 Train trainer/device_train_microbatch_size: 16
[stderr]: 	 Train loss/train/total: 2.7294
[stderr]: 	 Train loss/train/ce_loss: 2.7294
[stderr]: 	 Train metrics/train/LanguageCrossEntropy: 2.7294
[stderr]: 	 Train metrics/train/Perplexity: 15.3243
[stderr]: 	 Train metrics/train/cc_LanguageCrossEntropy: 2.5883
[stderr]: 	 Train metrics/train/cc_count: 111
[stderr]: 	 Train metrics/train/github_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/github_count: 0
[stderr]: 	 Train metrics/train/book_LanguageCrossEntropy: 2.7637
[stderr]: 	 Train metrics/train/book_count: 36
[stderr]: 	 Train metrics/train/stackexchange_LanguageCrossEntropy: 2.1109
[stderr]: 	 Train metrics/train/stackexchange_count: 3
[stderr]: 	 Train metrics/train/wiki_LanguageCrossEntropy: 2.2021
[stderr]: 	 Train metrics/train/wiki_count: 8
[stderr]: 	 Train metrics/train/arxiv_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/arxiv_count: 0
[stderr]: 	 Train metrics/train/c4-rp_LanguageCrossEntropy: 2.7929
[stderr]: 	 Train metrics/train/c4-rp_count: 354
[stderr]: 	 Train time/train: 0.0580
[stderr]: 	 Train time/val: 0.0000
[stderr]: 	 Train time/total: 0.0580
[stderr]: 	 Train lr-DecoupledAdamW/group0: 0.0000
[metric][batch=2]: time/batch: 2 
[metric][batch=2]: time/sample: 512 
[metric][batch=2]: time/batch_in_epoch: 2 
[metric][batch=2]: time/sample_in_epoch: 512 
[metric][batch=2]: time/token: 2097152 
[metric][batch=2]: time/token_in_epoch: 2097152 
[trace]: algorithm_traces/GradientClipping/Event.AFTER_TRAIN_BATCH:1
[metric][batch=2]: metrics/train/cc_weight: 0.2192 
[metric][batch=2]: metrics/train/github_weight: 0.0002 
[metric][batch=2]: metrics/train/book_weight: 0.0791 
[metric][batch=2]: metrics/train/stackexchange_weight: 0.0064 
[metric][batch=2]: metrics/train/wiki_weight: 0.0096 
[metric][batch=2]: metrics/train/arxiv_weight: 0.0010 
[metric][batch=2]: metrics/train/c4-rp_weight: 0.6845 
[metric][batch=2]: memory/current_allocated_mem: 25.8550 
[metric][batch=2]: memory/current_active_mem: 25.8550 
[metric][batch=2]: memory/current_inactive_mem: 0.5230 
[metric][batch=2]: memory/current_reserved_mem: 60.1190 
[metric][batch=2]: memory/peak_allocated_mem: 55.7030 
[metric][batch=2]: memory/peak_active_mem: 56.0670 
[metric][batch=2]: memory/peak_inactive_mem: 17.5110 
[metric][batch=2]: memory/peak_reserved_mem: 60.1190 
[metric][batch=2]: memory/alloc_retries: 0 
[metric][batch=2]: trainer/device_train_microbatch_size: 16 
[metric][batch=2]: loss/train/total: 2.8072 
[metric][batch=2]: loss/train/ce_loss: 2.8072 
[metric][batch=2]: metrics/train/LanguageCrossEntropy: 2.8072 
[metric][batch=2]: metrics/train/Perplexity: 16.5638 
[metric][batch=2]: metrics/train/cc_LanguageCrossEntropy: 2.6664 
[metric][batch=2]: metrics/train/cc_count: 152 
[metric][batch=2]: metrics/train/github_LanguageCrossEntropy: nan 
[metric][batch=2]: metrics/train/github_count: 0 
[metric][batch=2]: metrics/train/book_LanguageCrossEntropy: 2.8366 
[metric][batch=2]: metrics/train/book_count: 55 
[metric][batch=2]: metrics/train/stackexchange_LanguageCrossEntropy: 2.5546 
[metric][batch=2]: metrics/train/stackexchange_count: 4 
[metric][batch=2]: metrics/train/wiki_LanguageCrossEntropy: 2.0158 
[metric][batch=2]: metrics/train/wiki_count: 10 
[metric][batch=2]: metrics/train/arxiv_LanguageCrossEntropy: nan 
[metric][batch=2]: metrics/train/arxiv_count: 0 
[metric][batch=2]: metrics/train/c4-rp_LanguageCrossEntropy: 2.8437 
[metric][batch=2]: metrics/train/c4-rp_count: 547 
[metric][batch=3]: time/train: 0.0869 
[metric][batch=3]: time/val: 0.0000 
[metric][batch=3]: time/total: 0.0869 
[metric][batch=3]: lr-DecoupledAdamW/group0: 0.0000 
[stderr]: [batch=3/800]:
[stderr]: 	 Train time/batch: 2
[stderr]: 	 Train time/sample: 512
[stderr]: 	 Train time/batch_in_epoch: 2
[stderr]: 	 Train time/sample_in_epoch: 512
[stderr]: 	 Train time/token: 2097152
[stderr]: 	 Train time/token_in_epoch: 2097152
[stderr]: 	 Train metrics/train/cc_weight: 0.2192
[stderr]: 	 Train metrics/train/github_weight: 0.0002
[stderr]: 	 Train metrics/train/book_weight: 0.0791
[stderr]: 	 Train metrics/train/stackexchange_weight: 0.0064
[stderr]: 	 Train metrics/train/wiki_weight: 0.0096
[stderr]: 	 Train metrics/train/arxiv_weight: 0.0010
[stderr]: 	 Train metrics/train/c4-rp_weight: 0.6845
[stderr]: 	 Train memory/current_allocated_mem: 25.8550
[stderr]: 	 Train memory/current_active_mem: 25.8550
[stderr]: 	 Train memory/current_inactive_mem: 0.5230
[stderr]: 	 Train memory/current_reserved_mem: 60.1190
[stderr]: 	 Train memory/peak_allocated_mem: 55.7030
[stderr]: 	 Train memory/peak_active_mem: 56.0670
[stderr]: 	 Train memory/peak_inactive_mem: 17.5110
[stderr]: 	 Train memory/peak_reserved_mem: 60.1190
[stderr]: 	 Train memory/alloc_retries: 0
[stderr]: 	 Train trainer/device_train_microbatch_size: 16
[stderr]: 	 Train loss/train/total: 2.8072
[stderr]: 	 Train loss/train/ce_loss: 2.8072
[stderr]: 	 Train metrics/train/LanguageCrossEntropy: 2.8072
[stderr]: 	 Train metrics/train/Perplexity: 16.5638
[stderr]: 	 Train metrics/train/cc_LanguageCrossEntropy: 2.6664
[stderr]: 	 Train metrics/train/cc_count: 152
[stderr]: 	 Train metrics/train/github_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/github_count: 0
[stderr]: 	 Train metrics/train/book_LanguageCrossEntropy: 2.8366
[stderr]: 	 Train metrics/train/book_count: 55
[stderr]: 	 Train metrics/train/stackexchange_LanguageCrossEntropy: 2.5546
[stderr]: 	 Train metrics/train/stackexchange_count: 4
[stderr]: 	 Train metrics/train/wiki_LanguageCrossEntropy: 2.0158
[stderr]: 	 Train metrics/train/wiki_count: 10
[stderr]: 	 Train metrics/train/arxiv_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/arxiv_count: 0
[stderr]: 	 Train metrics/train/c4-rp_LanguageCrossEntropy: 2.8437
[stderr]: 	 Train metrics/train/c4-rp_count: 547
[stderr]: 	 Train time/train: 0.0869
[stderr]: 	 Train time/val: 0.0000
[stderr]: 	 Train time/total: 0.0869
[stderr]: 	 Train lr-DecoupledAdamW/group0: 0.0000
[metric][batch=3]: time/batch: 3 
[metric][batch=3]: time/sample: 768 
[metric][batch=3]: time/batch_in_epoch: 3 
[metric][batch=3]: time/sample_in_epoch: 768 
[metric][batch=3]: time/token: 3145728 
[metric][batch=3]: time/token_in_epoch: 3145728 
[trace]: algorithm_traces/GradientClipping/Event.AFTER_TRAIN_BATCH:1
[metric][batch=3]: metrics/train/cc_weight: 0.2192 
[metric][batch=3]: metrics/train/github_weight: 0.0002 
[metric][batch=3]: metrics/train/book_weight: 0.0791 
[metric][batch=3]: metrics/train/stackexchange_weight: 0.0064 
[metric][batch=3]: metrics/train/wiki_weight: 0.0096 
[metric][batch=3]: metrics/train/arxiv_weight: 0.0010 
[metric][batch=3]: metrics/train/c4-rp_weight: 0.6845 
[metric][batch=3]: memory/current_allocated_mem: 25.8550 
[metric][batch=3]: memory/current_active_mem: 25.8550 
[metric][batch=3]: memory/current_inactive_mem: 0.5230 
[metric][batch=3]: memory/current_reserved_mem: 60.1190 
[metric][batch=3]: memory/peak_allocated_mem: 55.7040 
[metric][batch=3]: memory/peak_active_mem: 56.0670 
[metric][batch=3]: memory/peak_inactive_mem: 17.5110 
[metric][batch=3]: memory/peak_reserved_mem: 60.1190 
[metric][batch=3]: memory/alloc_retries: 0 
[metric][batch=3]: trainer/device_train_microbatch_size: 16 
[metric][batch=3]: loss/train/total: 2.7360 
[metric][batch=3]: loss/train/ce_loss: 2.7360 
[metric][batch=3]: metrics/train/LanguageCrossEntropy: 2.7360 
[metric][batch=3]: metrics/train/Perplexity: 15.4255 
[metric][batch=3]: metrics/train/cc_LanguageCrossEntropy: 2.6327 
[metric][batch=3]: metrics/train/cc_count: 213 
[metric][batch=3]: metrics/train/github_LanguageCrossEntropy: nan 
[metric][batch=3]: metrics/train/github_count: 0 
[metric][batch=3]: metrics/train/book_LanguageCrossEntropy: 2.8399 
[metric][batch=3]: metrics/train/book_count: 71 
[metric][batch=3]: metrics/train/stackexchange_LanguageCrossEntropy: 2.0426 
[metric][batch=3]: metrics/train/stackexchange_count: 5 
[metric][batch=3]: metrics/train/wiki_LanguageCrossEntropy: 2.1877 
[metric][batch=3]: metrics/train/wiki_count: 15 
[metric][batch=3]: metrics/train/arxiv_LanguageCrossEntropy: nan 
[metric][batch=3]: metrics/train/arxiv_count: 0 
[metric][batch=3]: metrics/train/c4-rp_LanguageCrossEntropy: 2.7827 
[metric][batch=3]: metrics/train/c4-rp_count: 720 
[metric][batch=4]: time/train: 0.1158 
[metric][batch=4]: time/val: 0.0000 
[metric][batch=4]: time/total: 0.1158 
[metric][batch=4]: lr-DecoupledAdamW/group0: 0.0000 
[stderr]: [batch=4/800]:
[stderr]: 	 Train time/batch: 3
[stderr]: 	 Train time/sample: 768
[stderr]: 	 Train time/batch_in_epoch: 3
[stderr]: 	 Train time/sample_in_epoch: 768
[stderr]: 	 Train time/token: 3145728
[stderr]: 	 Train time/token_in_epoch: 3145728
[stderr]: 	 Train metrics/train/cc_weight: 0.2192
[stderr]: 	 Train metrics/train/github_weight: 0.0002
[stderr]: 	 Train metrics/train/book_weight: 0.0791
[stderr]: 	 Train metrics/train/stackexchange_weight: 0.0064
[stderr]: 	 Train metrics/train/wiki_weight: 0.0096
[stderr]: 	 Train metrics/train/arxiv_weight: 0.0010
[stderr]: 	 Train metrics/train/c4-rp_weight: 0.6845
[stderr]: 	 Train memory/current_allocated_mem: 25.8550
[stderr]: 	 Train memory/current_active_mem: 25.8550
[stderr]: 	 Train memory/current_inactive_mem: 0.5230
[stderr]: 	 Train memory/current_reserved_mem: 60.1190
[stderr]: 	 Train memory/peak_allocated_mem: 55.7040
[stderr]: 	 Train memory/peak_active_mem: 56.0670
[stderr]: 	 Train memory/peak_inactive_mem: 17.5110
[stderr]: 	 Train memory/peak_reserved_mem: 60.1190
[stderr]: 	 Train memory/alloc_retries: 0
[stderr]: 	 Train trainer/device_train_microbatch_size: 16
[stderr]: 	 Train loss/train/total: 2.7360
[stderr]: 	 Train loss/train/ce_loss: 2.7360
[stderr]: 	 Train metrics/train/LanguageCrossEntropy: 2.7360
[stderr]: 	 Train metrics/train/Perplexity: 15.4255
[stderr]: 	 Train metrics/train/cc_LanguageCrossEntropy: 2.6327
[stderr]: 	 Train metrics/train/cc_count: 213
[stderr]: 	 Train metrics/train/github_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/github_count: 0
[stderr]: 	 Train metrics/train/book_LanguageCrossEntropy: 2.8399
[stderr]: 	 Train metrics/train/book_count: 71
[stderr]: 	 Train metrics/train/stackexchange_LanguageCrossEntropy: 2.0426
[stderr]: 	 Train metrics/train/stackexchange_count: 5
[stderr]: 	 Train metrics/train/wiki_LanguageCrossEntropy: 2.1877
[stderr]: 	 Train metrics/train/wiki_count: 15
[stderr]: 	 Train metrics/train/arxiv_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/arxiv_count: 0
[stderr]: 	 Train metrics/train/c4-rp_LanguageCrossEntropy: 2.7827
[stderr]: 	 Train metrics/train/c4-rp_count: 720
[stderr]: 	 Train time/train: 0.1158
[stderr]: 	 Train time/val: 0.0000
[stderr]: 	 Train time/total: 0.1158
[stderr]: 	 Train lr-DecoupledAdamW/group0: 0.0000
[metric][batch=4]: time/batch: 4 
[metric][batch=4]: time/sample: 1024 
[metric][batch=4]: time/batch_in_epoch: 4 
[metric][batch=4]: time/sample_in_epoch: 1024 
[metric][batch=4]: time/token: 4194304 
[metric][batch=4]: time/token_in_epoch: 4194304 
[trace]: algorithm_traces/GradientClipping/Event.AFTER_TRAIN_BATCH:1
[metric][batch=4]: metrics/train/cc_weight: 0.2192 
[metric][batch=4]: metrics/train/github_weight: 0.0002 
[metric][batch=4]: metrics/train/book_weight: 0.0791 
[metric][batch=4]: metrics/train/stackexchange_weight: 0.0064 
[metric][batch=4]: metrics/train/wiki_weight: 0.0096 
[metric][batch=4]: metrics/train/arxiv_weight: 0.0010 
[metric][batch=4]: metrics/train/c4-rp_weight: 0.6845 
[metric][batch=4]: memory/current_allocated_mem: 25.8550 
[metric][batch=4]: memory/current_active_mem: 25.8550 
[metric][batch=4]: memory/current_inactive_mem: 0.5229 
[metric][batch=4]: memory/current_reserved_mem: 60.1190 
[metric][batch=4]: memory/peak_allocated_mem: 55.7040 
[metric][batch=4]: memory/peak_active_mem: 56.0670 
[metric][batch=4]: memory/peak_inactive_mem: 17.5110 
[metric][batch=4]: memory/peak_reserved_mem: 60.1190 
[metric][batch=4]: memory/alloc_retries: 0 
[metric][batch=4]: trainer/device_train_microbatch_size: 16 
[metric][batch=4]: loss/train/total: 2.7249 
[metric][batch=4]: loss/train/ce_loss: 2.7249 
[metric][batch=4]: metrics/train/LanguageCrossEntropy: 2.7249 
[metric][batch=4]: metrics/train/Perplexity: 15.2548 
[metric][batch=4]: metrics/train/cc_LanguageCrossEntropy: 2.5190 
[metric][batch=4]: metrics/train/cc_count: 271 
[metric][batch=4]: metrics/train/github_LanguageCrossEntropy: nan 
[metric][batch=4]: metrics/train/github_count: 0 
[metric][batch=4]: metrics/train/book_LanguageCrossEntropy: 2.9639 
[metric][batch=4]: metrics/train/book_count: 93 
[metric][batch=4]: metrics/train/stackexchange_LanguageCrossEntropy: nan 
[metric][batch=4]: metrics/train/stackexchange_count: 5 
[metric][batch=4]: metrics/train/wiki_LanguageCrossEntropy: 1.9880 
[metric][batch=4]: metrics/train/wiki_count: 17 
[metric][batch=4]: metrics/train/arxiv_LanguageCrossEntropy: 1.6974 
[metric][batch=4]: metrics/train/arxiv_count: 2 
[metric][batch=4]: metrics/train/c4-rp_LanguageCrossEntropy: 2.7843 
[metric][batch=4]: metrics/train/c4-rp_count: 892 
[metric][batch=5]: time/train: 0.1446 
[metric][batch=5]: time/val: 0.0000 
[metric][batch=5]: time/total: 0.1446 
[metric][batch=5]: lr-DecoupledAdamW/group0: 0.0000 
[stderr]: [batch=5/800]:
[stderr]: 	 Train time/batch: 4
[stderr]: 	 Train time/sample: 1024
[stderr]: 	 Train time/batch_in_epoch: 4
[stderr]: 	 Train time/sample_in_epoch: 1024
[stderr]: 	 Train time/token: 4194304
[stderr]: 	 Train time/token_in_epoch: 4194304
[stderr]: 	 Train metrics/train/cc_weight: 0.2192
[stderr]: 	 Train metrics/train/github_weight: 0.0002
[stderr]: 	 Train metrics/train/book_weight: 0.0791
[stderr]: 	 Train metrics/train/stackexchange_weight: 0.0064
[stderr]: 	 Train metrics/train/wiki_weight: 0.0096
[stderr]: 	 Train metrics/train/arxiv_weight: 0.0010
[stderr]: 	 Train metrics/train/c4-rp_weight: 0.6845
[stderr]: 	 Train memory/current_allocated_mem: 25.8550
[stderr]: 	 Train memory/current_active_mem: 25.8550
[stderr]: 	 Train memory/current_inactive_mem: 0.5229
[stderr]: 	 Train memory/current_reserved_mem: 60.1190
[stderr]: 	 Train memory/peak_allocated_mem: 55.7040
[stderr]: 	 Train memory/peak_active_mem: 56.0670
[stderr]: 	 Train memory/peak_inactive_mem: 17.5110
[stderr]: 	 Train memory/peak_reserved_mem: 60.1190
[stderr]: 	 Train memory/alloc_retries: 0
[stderr]: 	 Train trainer/device_train_microbatch_size: 16
[stderr]: 	 Train loss/train/total: 2.7249
[stderr]: 	 Train loss/train/ce_loss: 2.7249
[stderr]: 	 Train metrics/train/LanguageCrossEntropy: 2.7249
[stderr]: 	 Train metrics/train/Perplexity: 15.2548
[stderr]: 	 Train metrics/train/cc_LanguageCrossEntropy: 2.5190
[stderr]: 	 Train metrics/train/cc_count: 271
[stderr]: 	 Train metrics/train/github_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/github_count: 0
[stderr]: 	 Train metrics/train/book_LanguageCrossEntropy: 2.9639
[stderr]: 	 Train metrics/train/book_count: 93
[stderr]: 	 Train metrics/train/stackexchange_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/stackexchange_count: 5
[stderr]: 	 Train metrics/train/wiki_LanguageCrossEntropy: 1.9880
[stderr]: 	 Train metrics/train/wiki_count: 17
[stderr]: 	 Train metrics/train/arxiv_LanguageCrossEntropy: 1.6974
[stderr]: 	 Train metrics/train/arxiv_count: 2
[stderr]: 	 Train metrics/train/c4-rp_LanguageCrossEntropy: 2.7843
[stderr]: 	 Train metrics/train/c4-rp_count: 892
[stderr]: 	 Train time/train: 0.1446
[stderr]: 	 Train time/val: 0.0000
[stderr]: 	 Train time/total: 0.1446
[stderr]: 	 Train lr-DecoupledAdamW/group0: 0.0000
[stderr]: /opt/conda/lib/python3.10/site-packages/composer/core/state.py:1451: UserWarning: DataloaderNumBatchesWarning: The dataloader_len (1000) is greater than the length (i.e. number of batches) of the dataloader, which is 438. State.dataloader_len is thus being set to 438.
[stderr]:   warnings.warn((f'DataloaderNumBatchesWarning: The dataloader_len ({int(num_batches)}) '
[stderr]: /opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[stderr]:   warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[stderr]: [Eval batch=1/438] Eval on eval data
[stderr]: [Eval batch=45/438] Eval on eval data
[stderr]: [Eval batch=88/438] Eval on eval data
[stderr]: [Eval batch=132/438] Eval on eval data
[stderr]: [Eval batch=176/438] Eval on eval data
[stderr]: [Eval batch=220/438] Eval on eval data
[stderr]: [Eval batch=263/438] Eval on eval data
[stderr]: [Eval batch=307/438] Eval on eval data
[stderr]: [Eval batch=351/438] Eval on eval data
[stderr]: [Eval batch=394/438] Eval on eval data
[metric][batch=5]: metrics/eval/LanguageCrossEntropy: 2.3080 
[metric][batch=5]: metrics/eval/Perplexity: 10.0538 
[metric][batch=5]: metrics/eval/cc_LanguageCrossEntropy: 2.6087 
[metric][batch=5]: metrics/eval/github_LanguageCrossEntropy: 1.3485 
[metric][batch=5]: metrics/eval/book_LanguageCrossEntropy: 2.7958 
[metric][batch=5]: metrics/eval/stackexchange_LanguageCrossEntropy: 2.2459 
[metric][batch=5]: metrics/eval/wiki_LanguageCrossEntropy: 2.3096 
[metric][batch=5]: metrics/eval/arxiv_LanguageCrossEntropy: 2.0712 
[metric][batch=5]: metrics/eval/c4-rp_LanguageCrossEntropy: 2.7889 
[stderr]: [Eval batch=438/438] Eval on eval data:
[stderr]: 	 Eval metrics/eval/LanguageCrossEntropy: 2.3080
[stderr]: 	 Eval metrics/eval/Perplexity: 10.0538
[stderr]: 	 Eval metrics/eval/cc_LanguageCrossEntropy: 2.6087
[stderr]: 	 Eval metrics/eval/github_LanguageCrossEntropy: 1.3485
[stderr]: 	 Eval metrics/eval/book_LanguageCrossEntropy: 2.7958
[stderr]: 	 Eval metrics/eval/stackexchange_LanguageCrossEntropy: 2.2459
[stderr]: 	 Eval metrics/eval/wiki_LanguageCrossEntropy: 2.3096
[stderr]: 	 Eval metrics/eval/arxiv_LanguageCrossEntropy: 2.0712
[stderr]: 	 Eval metrics/eval/c4-rp_LanguageCrossEntropy: 2.7889
[metric][batch=5]: time/batch: 5 
[metric][batch=5]: time/sample: 1280 
[metric][batch=5]: time/batch_in_epoch: 5 
[metric][batch=5]: time/sample_in_epoch: 1280 
[metric][batch=5]: time/token: 5242880 
[metric][batch=5]: time/token_in_epoch: 5242880 
[trace]: algorithm_traces/GradientClipping/Event.AFTER_TRAIN_BATCH:1
[metric][batch=5]: metrics/train/cc_weight: 0.2134 
[metric][batch=5]: metrics/train/github_weight: 0.0002 
[metric][batch=5]: metrics/train/book_weight: 0.0789 
[metric][batch=5]: metrics/train/stackexchange_weight: 0.0061 
[metric][batch=5]: metrics/train/wiki_weight: 0.0077 
[metric][batch=5]: metrics/train/arxiv_weight: 0.0009 
[metric][batch=5]: metrics/train/c4-rp_weight: 0.6928 
[metric][batch=5]: memory/current_allocated_mem: 25.8550 
[metric][batch=5]: memory/current_active_mem: 25.8550 
[metric][batch=5]: memory/current_inactive_mem: 0.5229 
[metric][batch=5]: memory/current_reserved_mem: 68.5080 
[metric][batch=5]: memory/peak_allocated_mem: 55.7040 
[metric][batch=5]: memory/peak_active_mem: 56.0670 
[metric][batch=5]: memory/peak_inactive_mem: 17.5110 
[metric][batch=5]: memory/peak_reserved_mem: 68.5080 
[metric][batch=5]: memory/alloc_retries: 0 
[metric][batch=5]: trainer/device_train_microbatch_size: 16 
[metric][batch=5]: loss/train/total: 2.7582 
[metric][batch=5]: loss/train/ce_loss: 2.7582 
[metric][batch=5]: metrics/train/LanguageCrossEntropy: 2.7582 
[metric][batch=5]: metrics/train/Perplexity: 15.7722 
[metric][batch=5]: metrics/train/cc_LanguageCrossEntropy: 2.6377 
[metric][batch=5]: metrics/train/cc_count: 319 
[metric][batch=5]: metrics/train/github_LanguageCrossEntropy: nan 
[metric][batch=5]: metrics/train/github_count: 0 
[metric][batch=5]: metrics/train/book_LanguageCrossEntropy: 2.7170 
[metric][batch=5]: metrics/train/book_count: 116 
[metric][batch=5]: metrics/train/stackexchange_LanguageCrossEntropy: 2.0943 
[metric][batch=5]: metrics/train/stackexchange_count: 7 
[metric][batch=5]: metrics/train/wiki_LanguageCrossEntropy: 2.4243 
[metric][batch=5]: metrics/train/wiki_count: 18 
[metric][batch=5]: metrics/train/arxiv_LanguageCrossEntropy: nan 
[metric][batch=5]: metrics/train/arxiv_count: 2 
[metric][batch=5]: metrics/train/c4-rp_LanguageCrossEntropy: 2.8044 
[metric][batch=5]: metrics/train/c4-rp_count: 1074 
[metric][batch=6]: time/train: 0.1735 
[metric][batch=6]: time/val: 0.1199 
[metric][batch=6]: time/total: 0.2933 
[metric][batch=6]: lr-DecoupledAdamW/group0: 0.0000 
[stderr]: [batch=6/800]:
[stderr]: 	 Train time/batch: 5
[stderr]: 	 Train time/sample: 1280
[stderr]: 	 Train time/batch_in_epoch: 5
[stderr]: 	 Train time/sample_in_epoch: 1280
[stderr]: 	 Train time/token: 5242880
[stderr]: 	 Train time/token_in_epoch: 5242880
[stderr]: 	 Train metrics/train/cc_weight: 0.2134
[stderr]: 	 Train metrics/train/github_weight: 0.0002
[stderr]: 	 Train metrics/train/book_weight: 0.0789
[stderr]: 	 Train metrics/train/stackexchange_weight: 0.0061
[stderr]: 	 Train metrics/train/wiki_weight: 0.0077
[stderr]: 	 Train metrics/train/arxiv_weight: 0.0009
[stderr]: 	 Train metrics/train/c4-rp_weight: 0.6928
[stderr]: 	 Train memory/current_allocated_mem: 25.8550
[stderr]: 	 Train memory/current_active_mem: 25.8550
[stderr]: 	 Train memory/current_inactive_mem: 0.5229
[stderr]: 	 Train memory/current_reserved_mem: 68.5080
[stderr]: 	 Train memory/peak_allocated_mem: 55.7040
[stderr]: 	 Train memory/peak_active_mem: 56.0670
[stderr]: 	 Train memory/peak_inactive_mem: 17.5110
[stderr]: 	 Train memory/peak_reserved_mem: 68.5080
[stderr]: 	 Train memory/alloc_retries: 0
[stderr]: 	 Train trainer/device_train_microbatch_size: 16
[stderr]: 	 Train loss/train/total: 2.7582
[stderr]: 	 Train loss/train/ce_loss: 2.7582
[stderr]: 	 Train metrics/train/LanguageCrossEntropy: 2.7582
[stderr]: 	 Train metrics/train/Perplexity: 15.7722
[stderr]: 	 Train metrics/train/cc_LanguageCrossEntropy: 2.6377
[stderr]: 	 Train metrics/train/cc_count: 319
[stderr]: 	 Train metrics/train/github_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/github_count: 0
[stderr]: 	 Train metrics/train/book_LanguageCrossEntropy: 2.7170
[stderr]: 	 Train metrics/train/book_count: 116
[stderr]: 	 Train metrics/train/stackexchange_LanguageCrossEntropy: 2.0943
[stderr]: 	 Train metrics/train/stackexchange_count: 7
[stderr]: 	 Train metrics/train/wiki_LanguageCrossEntropy: 2.4243
[stderr]: 	 Train metrics/train/wiki_count: 18
[stderr]: 	 Train metrics/train/arxiv_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/arxiv_count: 2
[stderr]: 	 Train metrics/train/c4-rp_LanguageCrossEntropy: 2.8044
[stderr]: 	 Train metrics/train/c4-rp_count: 1074
[stderr]: 	 Train time/train: 0.1735
[stderr]: 	 Train time/val: 0.1199
[stderr]: 	 Train time/total: 0.2933
[stderr]: 	 Train lr-DecoupledAdamW/group0: 0.0000
[metric][batch=6]: time/batch: 6 
[metric][batch=6]: time/sample: 1536 
[metric][batch=6]: time/batch_in_epoch: 6 
[metric][batch=6]: time/sample_in_epoch: 1536 
[metric][batch=6]: time/token: 6291456 
[metric][batch=6]: time/token_in_epoch: 6291456 
[trace]: algorithm_traces/GradientClipping/Event.AFTER_TRAIN_BATCH:1
[metric][batch=6]: metrics/train/cc_weight: 0.2134 
[metric][batch=6]: metrics/train/github_weight: 0.0002 
[metric][batch=6]: metrics/train/book_weight: 0.0789 
[metric][batch=6]: metrics/train/stackexchange_weight: 0.0061 
[metric][batch=6]: metrics/train/wiki_weight: 0.0077 
[metric][batch=6]: metrics/train/arxiv_weight: 0.0009 
[metric][batch=6]: metrics/train/c4-rp_weight: 0.6928 
[metric][batch=6]: memory/current_allocated_mem: 25.8550 
[metric][batch=6]: memory/current_active_mem: 25.8550 
[metric][batch=6]: memory/current_inactive_mem: 0.5229 
[metric][batch=6]: memory/current_reserved_mem: 68.5080 
[metric][batch=6]: memory/peak_allocated_mem: 55.7040 
[metric][batch=6]: memory/peak_active_mem: 56.0670 
[metric][batch=6]: memory/peak_inactive_mem: 17.5110 
[metric][batch=6]: memory/peak_reserved_mem: 68.5080 
[metric][batch=6]: memory/alloc_retries: 0 
[metric][batch=6]: trainer/device_train_microbatch_size: 16 
[metric][batch=6]: loss/train/total: 2.7457 
[metric][batch=6]: loss/train/ce_loss: 2.7457 
[metric][batch=6]: metrics/train/LanguageCrossEntropy: 2.7457 
[metric][batch=6]: metrics/train/Perplexity: 15.5750 
[metric][batch=6]: metrics/train/cc_LanguageCrossEntropy: 2.6087 
[metric][batch=6]: metrics/train/cc_count: 383 
[metric][batch=6]: metrics/train/github_LanguageCrossEntropy: nan 
[metric][batch=6]: metrics/train/github_count: 0 
[metric][batch=6]: metrics/train/book_LanguageCrossEntropy: 2.8913 
[metric][batch=6]: metrics/train/book_count: 138 
[metric][batch=6]: metrics/train/stackexchange_LanguageCrossEntropy: 1.8315 
[metric][batch=6]: metrics/train/stackexchange_count: 8 
[metric][batch=6]: metrics/train/wiki_LanguageCrossEntropy: 2.0097 
[metric][batch=6]: metrics/train/wiki_count: 19 
[metric][batch=6]: metrics/train/arxiv_LanguageCrossEntropy: nan 
[metric][batch=6]: metrics/train/arxiv_count: 2 
[metric][batch=6]: metrics/train/c4-rp_LanguageCrossEntropy: 2.7886 
[metric][batch=6]: metrics/train/c4-rp_count: 1242 
[metric][batch=7]: time/train: 0.2023 
[metric][batch=7]: time/val: 0.1199 
[metric][batch=7]: time/total: 0.3222 
[metric][batch=7]: lr-DecoupledAdamW/group0: 0.0000 
[stderr]: [batch=7/800]:
[stderr]: 	 Train time/batch: 6
[stderr]: 	 Train time/sample: 1536
[stderr]: 	 Train time/batch_in_epoch: 6
[stderr]: 	 Train time/sample_in_epoch: 1536
[stderr]: 	 Train time/token: 6291456
[stderr]: 	 Train time/token_in_epoch: 6291456
[stderr]: 	 Train metrics/train/cc_weight: 0.2134
[stderr]: 	 Train metrics/train/github_weight: 0.0002
[stderr]: 	 Train metrics/train/book_weight: 0.0789
[stderr]: 	 Train metrics/train/stackexchange_weight: 0.0061
[stderr]: 	 Train metrics/train/wiki_weight: 0.0077
[stderr]: 	 Train metrics/train/arxiv_weight: 0.0009
[stderr]: 	 Train metrics/train/c4-rp_weight: 0.6928
[stderr]: 	 Train memory/current_allocated_mem: 25.8550
[stderr]: 	 Train memory/current_active_mem: 25.8550
[stderr]: 	 Train memory/current_inactive_mem: 0.5229
[stderr]: 	 Train memory/current_reserved_mem: 68.5080
[stderr]: 	 Train memory/peak_allocated_mem: 55.7040
[stderr]: 	 Train memory/peak_active_mem: 56.0670
[stderr]: 	 Train memory/peak_inactive_mem: 17.5110
[stderr]: 	 Train memory/peak_reserved_mem: 68.5080
[stderr]: 	 Train memory/alloc_retries: 0
[stderr]: 	 Train trainer/device_train_microbatch_size: 16
[stderr]: 	 Train loss/train/total: 2.7457
[stderr]: 	 Train loss/train/ce_loss: 2.7457
[stderr]: 	 Train metrics/train/LanguageCrossEntropy: 2.7457
[stderr]: 	 Train metrics/train/Perplexity: 15.5750
[stderr]: 	 Train metrics/train/cc_LanguageCrossEntropy: 2.6087
[stderr]: 	 Train metrics/train/cc_count: 383
[stderr]: 	 Train metrics/train/github_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/github_count: 0
[stderr]: 	 Train metrics/train/book_LanguageCrossEntropy: 2.8913
[stderr]: 	 Train metrics/train/book_count: 138
[stderr]: 	 Train metrics/train/stackexchange_LanguageCrossEntropy: 1.8315
[stderr]: 	 Train metrics/train/stackexchange_count: 8
[stderr]: 	 Train metrics/train/wiki_LanguageCrossEntropy: 2.0097
[stderr]: 	 Train metrics/train/wiki_count: 19
[stderr]: 	 Train metrics/train/arxiv_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/arxiv_count: 2
[stderr]: 	 Train metrics/train/c4-rp_LanguageCrossEntropy: 2.7886
[stderr]: 	 Train metrics/train/c4-rp_count: 1242
[stderr]: 	 Train time/train: 0.2023
[stderr]: 	 Train time/val: 0.1199
[stderr]: 	 Train time/total: 0.3222
[stderr]: 	 Train lr-DecoupledAdamW/group0: 0.0000
[metric][batch=7]: time/batch: 7 
[metric][batch=7]: time/sample: 1792 
[metric][batch=7]: time/batch_in_epoch: 7 
[metric][batch=7]: time/sample_in_epoch: 1792 
[metric][batch=7]: time/token: 7340032 
[metric][batch=7]: time/token_in_epoch: 7340032 
[trace]: algorithm_traces/GradientClipping/Event.AFTER_TRAIN_BATCH:1
[metric][batch=7]: metrics/train/cc_weight: 0.2134 
[metric][batch=7]: metrics/train/github_weight: 0.0002 
[metric][batch=7]: metrics/train/book_weight: 0.0789 
[metric][batch=7]: metrics/train/stackexchange_weight: 0.0061 
[metric][batch=7]: metrics/train/wiki_weight: 0.0077 
[metric][batch=7]: metrics/train/arxiv_weight: 0.0009 
[metric][batch=7]: metrics/train/c4-rp_weight: 0.6928 
[metric][batch=7]: memory/current_allocated_mem: 25.8550 
[metric][batch=7]: memory/current_active_mem: 25.8550 
[metric][batch=7]: memory/current_inactive_mem: 0.5228 
[metric][batch=7]: memory/current_reserved_mem: 68.5080 
[metric][batch=7]: memory/peak_allocated_mem: 55.7040 
[metric][batch=7]: memory/peak_active_mem: 56.0670 
[metric][batch=7]: memory/peak_inactive_mem: 17.5110 
[metric][batch=7]: memory/peak_reserved_mem: 68.5080 
[metric][batch=7]: memory/alloc_retries: 0 
[metric][batch=7]: trainer/device_train_microbatch_size: 16 
[metric][batch=7]: loss/train/total: 2.7310 
[metric][batch=7]: loss/train/ce_loss: 2.7310 
[metric][batch=7]: metrics/train/LanguageCrossEntropy: 2.7310 
[metric][batch=7]: metrics/train/Perplexity: 15.3477 
[metric][batch=7]: metrics/train/cc_LanguageCrossEntropy: 2.5641 
[metric][batch=7]: metrics/train/cc_count: 424 
[metric][batch=7]: metrics/train/github_LanguageCrossEntropy: nan 
[metric][batch=7]: metrics/train/github_count: 0 
[metric][batch=7]: metrics/train/book_LanguageCrossEntropy: 2.7349 
[metric][batch=7]: metrics/train/book_count: 158 
[metric][batch=7]: metrics/train/stackexchange_LanguageCrossEntropy: nan 
[metric][batch=7]: metrics/train/stackexchange_count: 8 
[metric][batch=7]: metrics/train/wiki_LanguageCrossEntropy: nan 
[metric][batch=7]: metrics/train/wiki_count: 19 
[metric][batch=7]: metrics/train/arxiv_LanguageCrossEntropy: nan 
[metric][batch=7]: metrics/train/arxiv_count: 2 
[metric][batch=7]: metrics/train/c4-rp_LanguageCrossEntropy: 2.7656 
[metric][batch=7]: metrics/train/c4-rp_count: 1437 
[metric][batch=8]: time/train: 0.2311 
[metric][batch=8]: time/val: 0.1199 
[metric][batch=8]: time/total: 0.3510 
[metric][batch=8]: lr-DecoupledAdamW/group0: 0.0000 
[stderr]: [batch=8/800]:
[stderr]: 	 Train time/batch: 7
[stderr]: 	 Train time/sample: 1792
[stderr]: 	 Train time/batch_in_epoch: 7
[stderr]: 	 Train time/sample_in_epoch: 1792
[stderr]: 	 Train time/token: 7340032
[stderr]: 	 Train time/token_in_epoch: 7340032
[stderr]: 	 Train metrics/train/cc_weight: 0.2134
[stderr]: 	 Train metrics/train/github_weight: 0.0002
[stderr]: 	 Train metrics/train/book_weight: 0.0789
[stderr]: 	 Train metrics/train/stackexchange_weight: 0.0061
[stderr]: 	 Train metrics/train/wiki_weight: 0.0077
[stderr]: 	 Train metrics/train/arxiv_weight: 0.0009
[stderr]: 	 Train metrics/train/c4-rp_weight: 0.6928
[stderr]: 	 Train memory/current_allocated_mem: 25.8550
[stderr]: 	 Train memory/current_active_mem: 25.8550
[stderr]: 	 Train memory/current_inactive_mem: 0.5228
[stderr]: 	 Train memory/current_reserved_mem: 68.5080
[stderr]: 	 Train memory/peak_allocated_mem: 55.7040
[stderr]: 	 Train memory/peak_active_mem: 56.0670
[stderr]: 	 Train memory/peak_inactive_mem: 17.5110
[stderr]: 	 Train memory/peak_reserved_mem: 68.5080
[stderr]: 	 Train memory/alloc_retries: 0
[stderr]: 	 Train trainer/device_train_microbatch_size: 16
[stderr]: 	 Train loss/train/total: 2.7310
[stderr]: 	 Train loss/train/ce_loss: 2.7310
[stderr]: 	 Train metrics/train/LanguageCrossEntropy: 2.7310
[stderr]: 	 Train metrics/train/Perplexity: 15.3477
[stderr]: 	 Train metrics/train/cc_LanguageCrossEntropy: 2.5641
[stderr]: 	 Train metrics/train/cc_count: 424
[stderr]: 	 Train metrics/train/github_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/github_count: 0
[stderr]: 	 Train metrics/train/book_LanguageCrossEntropy: 2.7349
[stderr]: 	 Train metrics/train/book_count: 158
[stderr]: 	 Train metrics/train/stackexchange_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/stackexchange_count: 8
[stderr]: 	 Train metrics/train/wiki_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/wiki_count: 19
[stderr]: 	 Train metrics/train/arxiv_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/arxiv_count: 2
[stderr]: 	 Train metrics/train/c4-rp_LanguageCrossEntropy: 2.7656
[stderr]: 	 Train metrics/train/c4-rp_count: 1437
[stderr]: 	 Train time/train: 0.2311
[stderr]: 	 Train time/val: 0.1199
[stderr]: 	 Train time/total: 0.3510
[stderr]: 	 Train lr-DecoupledAdamW/group0: 0.0000
[metric][batch=8]: time/batch: 8 
[metric][batch=8]: time/sample: 2048 
[metric][batch=8]: time/batch_in_epoch: 8 
[metric][batch=8]: time/sample_in_epoch: 2048 
[metric][batch=8]: time/token: 8388608 
[metric][batch=8]: time/token_in_epoch: 8388608 
[trace]: algorithm_traces/GradientClipping/Event.AFTER_TRAIN_BATCH:1
[metric][batch=8]: metrics/train/cc_weight: 0.2134 
[metric][batch=8]: metrics/train/github_weight: 0.0002 
[metric][batch=8]: metrics/train/book_weight: 0.0789 
[metric][batch=8]: metrics/train/stackexchange_weight: 0.0061 
[metric][batch=8]: metrics/train/wiki_weight: 0.0077 
[metric][batch=8]: metrics/train/arxiv_weight: 0.0009 
[metric][batch=8]: metrics/train/c4-rp_weight: 0.6928 
[metric][batch=8]: memory/current_allocated_mem: 25.8550 
[metric][batch=8]: memory/current_active_mem: 25.8550 
[metric][batch=8]: memory/current_inactive_mem: 0.5228 
[metric][batch=8]: memory/current_reserved_mem: 68.5080 
[metric][batch=8]: memory/peak_allocated_mem: 55.7040 
[metric][batch=8]: memory/peak_active_mem: 56.0670 
[metric][batch=8]: memory/peak_inactive_mem: 17.5110 
[metric][batch=8]: memory/peak_reserved_mem: 68.5080 
[metric][batch=8]: memory/alloc_retries: 0 
[metric][batch=8]: trainer/device_train_microbatch_size: 16 
[metric][batch=8]: loss/train/total: 2.7628 
[metric][batch=8]: loss/train/ce_loss: 2.7628 
[metric][batch=8]: metrics/train/LanguageCrossEntropy: 2.7629 
[metric][batch=8]: metrics/train/Perplexity: 15.8449 
[metric][batch=8]: metrics/train/cc_LanguageCrossEntropy: 2.6190 
[metric][batch=8]: metrics/train/cc_count: 485 
[metric][batch=8]: metrics/train/github_LanguageCrossEntropy: nan 
[metric][batch=8]: metrics/train/github_count: 0 
[metric][batch=8]: metrics/train/book_LanguageCrossEntropy: 2.7550 
[metric][batch=8]: metrics/train/book_count: 173 
[metric][batch=8]: metrics/train/stackexchange_LanguageCrossEntropy: nan 
[metric][batch=8]: metrics/train/stackexchange_count: 8 
[metric][batch=8]: metrics/train/wiki_LanguageCrossEntropy: 2.1459 
[metric][batch=8]: metrics/train/wiki_count: 23 
[metric][batch=8]: metrics/train/arxiv_LanguageCrossEntropy: nan 
[metric][batch=8]: metrics/train/arxiv_count: 2 
[metric][batch=8]: metrics/train/c4-rp_LanguageCrossEntropy: 2.8274 
[metric][batch=8]: metrics/train/c4-rp_count: 1613 
[metric][batch=9]: time/train: 0.2600 
[metric][batch=9]: time/val: 0.1199 
[metric][batch=9]: time/total: 0.3799 
[metric][batch=9]: lr-DecoupledAdamW/group0: 0.0000 
[stderr]: [batch=9/800]:
[stderr]: 	 Train time/batch: 8
[stderr]: 	 Train time/sample: 2048
[stderr]: 	 Train time/batch_in_epoch: 8
[stderr]: 	 Train time/sample_in_epoch: 2048
[stderr]: 	 Train time/token: 8388608
[stderr]: 	 Train time/token_in_epoch: 8388608
[stderr]: 	 Train metrics/train/cc_weight: 0.2134
[stderr]: 	 Train metrics/train/github_weight: 0.0002
[stderr]: 	 Train metrics/train/book_weight: 0.0789
[stderr]: 	 Train metrics/train/stackexchange_weight: 0.0061
[stderr]: 	 Train metrics/train/wiki_weight: 0.0077
[stderr]: 	 Train metrics/train/arxiv_weight: 0.0009
[stderr]: 	 Train metrics/train/c4-rp_weight: 0.6928
[stderr]: 	 Train memory/current_allocated_mem: 25.8550
[stderr]: 	 Train memory/current_active_mem: 25.8550
[stderr]: 	 Train memory/current_inactive_mem: 0.5228
[stderr]: 	 Train memory/current_reserved_mem: 68.5080
[stderr]: 	 Train memory/peak_allocated_mem: 55.7040
[stderr]: 	 Train memory/peak_active_mem: 56.0670
[stderr]: 	 Train memory/peak_inactive_mem: 17.5110
[stderr]: 	 Train memory/peak_reserved_mem: 68.5080
[stderr]: 	 Train memory/alloc_retries: 0
[stderr]: 	 Train trainer/device_train_microbatch_size: 16
[stderr]: 	 Train loss/train/total: 2.7628
[stderr]: 	 Train loss/train/ce_loss: 2.7628
[stderr]: 	 Train metrics/train/LanguageCrossEntropy: 2.7629
[stderr]: 	 Train metrics/train/Perplexity: 15.8449
[stderr]: 	 Train metrics/train/cc_LanguageCrossEntropy: 2.6190
[stderr]: 	 Train metrics/train/cc_count: 485
[stderr]: 	 Train metrics/train/github_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/github_count: 0
[stderr]: 	 Train metrics/train/book_LanguageCrossEntropy: 2.7550
[stderr]: 	 Train metrics/train/book_count: 173
[stderr]: 	 Train metrics/train/stackexchange_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/stackexchange_count: 8
[stderr]: 	 Train metrics/train/wiki_LanguageCrossEntropy: 2.1459
[stderr]: 	 Train metrics/train/wiki_count: 23
[stderr]: 	 Train metrics/train/arxiv_LanguageCrossEntropy: nan
[stderr]: 	 Train metrics/train/arxiv_count: 2
[stderr]: 	 Train metrics/train/c4-rp_LanguageCrossEntropy: 2.8274
[stderr]: 	 Train metrics/train/c4-rp_count: 1613
[stderr]: 	 Train time/train: 0.2600
[stderr]: 	 Train time/val: 0.1199
[stderr]: 	 Train time/total: 0.3799
[stderr]: 	 Train lr-DecoupledAdamW/group0: 0.0000
[metric][batch=9]: time/batch: 9 
[metric][batch=9]: time/sample: 2304 
[metric][batch=9]: time/batch_in_epoch: 9 
[metric][batch=9]: time/sample_in_epoch: 2304 
[metric][batch=9]: time/token: 9437184 
[metric][batch=9]: time/token_in_epoch: 9437184 
[stderr]: Traceback (most recent call last):
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/trainer/trainer.py", line 2468, in _train_microbatch
[stderr]:     microbatch_loss.backward(create_graph=self._backwards_create_graph)
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
[stderr]:     torch.autograd.backward(
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
[stderr]:     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/core/engine.py", line 123, in sigterm_handler
[stderr]:     sys.exit(128 + signal)
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/exit_hooks.py", line 36, in exit
[stderr]:     self._orig_exit(orig_code)  # type: ignore
[stderr]: SystemExit: 130

[stderr]: During handling of the above exception, another exception occurred:

[stderr]: Traceback (most recent call last):
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/trainer/trainer.py", line 2339, in _train_microbatches
[stderr]:     microbatch_loss_dict = self._train_microbatch(use_grad_scaling, current_batch_size, is_final_microbatch)
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/trainer/trainer.py", line 2468, in _train_microbatch
[stderr]:     microbatch_loss.backward(create_graph=self._backwards_create_graph)
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/core/engine.py", line 123, in sigterm_handler
[stderr]:     sys.exit(128 + signal)
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/exit_hooks.py", line 36, in exit
[stderr]:     self._orig_exit(orig_code)  # type: ignore
[stderr]: SystemExit: 143

[stderr]: During handling of the above exception, another exception occurred:

[stderr]: Traceback (most recent call last):
[stderr]:   File "/home/shuyaoli/LLM-Shearing/llmshearing/train.py", line 343, in <module>
[stderr]:     main(cfg)
[stderr]:   File "/home/shuyaoli/LLM-Shearing/llmshearing/train.py", line 327, in main
[stderr]:     trainer.fit()
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/trainer/trainer.py", line 1876, in fit
[stderr]:     self._train_loop()
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/trainer/trainer.py", line 2051, in _train_loop
[stderr]:     total_loss_dict = self._train_batch(use_grad_scaling)
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/trainer/trainer.py", line 2236, in _train_batch
[stderr]:     optimizer.step(closure=lambda loss_dict=total_loss_dict, **kwargs: self._train_microbatches(
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
[stderr]:     return wrapped(*args, **kwargs)
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
[stderr]:     out = func(*args, **kwargs)
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
[stderr]:     return func(*args, **kwargs)
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/optim/decoupled_weight_decay.py", line 276, in step
[stderr]:     loss = closure()
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/trainer/trainer.py", line 2236, in <lambda>
[stderr]:     optimizer.step(closure=lambda loss_dict=total_loss_dict, **kwargs: self._train_microbatches(
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/composer/trainer/trainer.py", line 2339, in _train_microbatches
[stderr]:     microbatch_loss_dict = self._train_microbatch(use_grad_scaling, current_batch_size, is_final_microbatch)
[stderr]:   File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
[stderr]:     _error_if_any_worker_fails()
[stderr]: RuntimeError: DataLoader worker (pid 10583) exited unexpectedly with exit code 130. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
[stderr]: [31m╭─[0m[31m────────────[0m[31m [0m[1;31mTraceback [0m[1;2;31m(most recent call last)[0m[31m [0m[31m─────────────[0m[31m─╮[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/trainer/[0m[1;33mtra[0m [31m│[0m
[stderr]: [31m│[0m [1;33miner.py[0m:[94m2468[0m in [92m_train_microbatch[0m                            [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m2465 [0m[2m│   │   │   [0m[94melse[0m:                                     [31m│[0m
[stderr]: [31m│[0m   [2m2466 [0m[2m│   │   │   │   [0m[2m# Scale loss based on the number of s[0m [31m│[0m
[stderr]: [31m│[0m   [2m2467 [0m[2m│   │   │   │   [0mmicrobatch_loss.mul_(microbatch_num_s [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m2468 [2m│   │   │   │   [0mmicrobatch_loss.backward(create_graph [31m│[0m
[stderr]: [31m│[0m   [2m2469 [0m[2m│   │   │   [0m                                          [31m│[0m
[stderr]: [31m│[0m   [2m2470 [0m[2m│   │   │   [0m[96mself[0m.engine.run_event(Event.AFTER_BACKWAR [31m│[0m
[stderr]: [31m│[0m   [2m2471 [0m                                                      [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/torch/[0m[1;33m_tensor.py[0m:[94m487[0m [31m│[0m
[stderr]: [31m│[0m in [92mbackward[0m                                                  [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m 484 [0m[2m│   │   │   │   [0mcreate_graph=create_graph,            [31m│[0m
[stderr]: [31m│[0m   [2m 485 [0m[2m│   │   │   │   [0minputs=inputs,                        [31m│[0m
[stderr]: [31m│[0m   [2m 486 [0m[2m│   │   │   [0m)                                         [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m 487 [2m│   │   [0mtorch.autograd.backward(                      [31m│[0m
[stderr]: [31m│[0m   [2m 488 [0m[2m│   │   │   [0m[96mself[0m, gradient, retain_graph, create_grap [31m│[0m
[stderr]: [31m│[0m   [2m 489 [0m[2m│   │   [0m)                                             [31m│[0m
[stderr]: [31m│[0m   [2m 490 [0m                                                      [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/torch/autograd/[0m[1;33m__ini[0m [31m│[0m
[stderr]: [31m│[0m [1;33mt__.py[0m:[94m200[0m in [92mbackward[0m                                       [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m197 [0m[2m│   [0m[2m# The reason we repeat same the comment below is t[0m [31m│[0m
[stderr]: [31m│[0m   [2m198 [0m[2m│   [0m[2m# some Python versions print out the first line of[0m [31m│[0m
[stderr]: [31m│[0m   [2m199 [0m[2m│   [0m[2m# calls in the traceback and some print out the la[0m [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m200 [2m│   [0mVariable._execution_engine.run_backward(  [2m# Calls [0m [31m│[0m
[stderr]: [31m│[0m   [2m201 [0m[2m│   │   [0mtensors, grad_tensors_, retain_graph, create_g [31m│[0m
[stderr]: [31m│[0m   [2m202 [0m[2m│   │   [0mallow_unreachable=[94mTrue[0m, accumulate_grad=[94mTrue[0m)  [31m│[0m
[stderr]: [31m│[0m   [2m203 [0m                                                       [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/core/[0m[1;33mengine[0m [31m│[0m
[stderr]: [31m│[0m [1;33m.py[0m:[94m123[0m in [92msigterm_handler[0m                                   [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m120 [0m[2m# functions still run. Composer CLI launcher will give[0m [31m│[0m
[stderr]: [31m│[0m   [2m121 [0m[2m# SIGKILL.[0m                                             [31m│[0m
[stderr]: [31m│[0m   [2m122 [0m[94mdef[0m [92msigterm_handler[0m(signal, frame):                    [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m123 [2m│   [0msys.exit([94m128[0m + signal)                             [31m│[0m
[stderr]: [31m│[0m   [2m124 [0m                                                       [31m│[0m
[stderr]: [31m│[0m   [2m125 [0m                                                       [31m│[0m
[stderr]: [31m│[0m   [2m126 [0msignal.signal(signal.SIGTERM, sigterm_handler)         [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/[0m[1;33mexit_h[0m [31m│[0m
[stderr]: [31m│[0m [1;33mooks.py[0m:[94m36[0m in [92mexit[0m                                           [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m33 [0m[2m│   │   [0mcode = code [94mif[0m code [95mis[0m [95mnot[0m [94mNone[0m [94melse[0m [94m0[0m          [31m│[0m
[stderr]: [31m│[0m   [2m34 [0m[2m│   │   [0mcode = code [94mif[0m [96misinstance[0m(code, [96mint[0m) [94melse[0m [94m1[0m     [31m│[0m
[stderr]: [31m│[0m   [2m35 [0m[2m│   │   [0m[96mself[0m.exit_code = code                           [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m36 [2m│   │   [0m[96mself[0m._orig_exit(orig_code)  [2m# type: ignore[0m      [31m│[0m
[stderr]: [31m│[0m   [2m37 [0m[2m│   [0m                                                    [31m│[0m
[stderr]: [31m│[0m   [2m38 [0m[2m│   [0m[94mdef[0m [92mwas_ctrl_c[0m([96mself[0m) -> [96mbool[0m:                       [31m│[0m
[stderr]: [31m│[0m   [2m39 [0m[2m│   │   [0m[94mreturn[0m [96misinstance[0m([96mself[0m.exception, [96mKeyboardInter[0m [31m│[0m
[stderr]: [31m╰──────────────────────────────────────────────────────────────╯[0m
[stderr]: [1;91mSystemExit: [0m[1;36m130[0m

[stderr]: [3mDuring handling of the above exception, another exception [0m
[stderr]: [3moccurred:[0m

[stderr]: [31m╭─[0m[31m────────────[0m[31m [0m[1;31mTraceback [0m[1;2;31m(most recent call last)[0m[31m [0m[31m─────────────[0m[31m─╮[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/trainer/[0m[1;33mtra[0m [31m│[0m
[stderr]: [31m│[0m [1;33miner.py[0m:[94m2339[0m in [92m_train_microbatches[0m                          [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m2336 [0m[2m│   │   │   [0m                                          [31m│[0m
[stderr]: [31m│[0m   [2m2337 [0m[2m│   │   │   [0m[94mfor[0m microbatch_idx, [96mself[0m.state.batch [95min[0m [96me[0m [31m│[0m
[stderr]: [31m│[0m   [2m2338 [0m[2m│   │   │   │   [0mis_final_microbatch = microbatch_idx  [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m2339 [2m│   │   │   │   [0mmicrobatch_loss_dict = [96mself[0m._train_mi [31m│[0m
[stderr]: [31m│[0m   [2m2340 [0m[2m│   │   │   │   [0m                                      [31m│[0m
[stderr]: [31m│[0m   [2m2341 [0m[2m│   │   │   │   [0m[2m# Aggregate each loss in microbatch_l[0m [31m│[0m
[stderr]: [31m│[0m   [2m2342 [0m[2m│   │   │   │   [0m[94mfor[0m k, microbatch_loss [95min[0m microbatch_ [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/trainer/[0m[1;33mtra[0m [31m│[0m
[stderr]: [31m│[0m [1;33miner.py[0m:[94m2468[0m in [92m_train_microbatch[0m                            [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m2465 [0m[2m│   │   │   [0m[94melse[0m:                                     [31m│[0m
[stderr]: [31m│[0m   [2m2466 [0m[2m│   │   │   │   [0m[2m# Scale loss based on the number of s[0m [31m│[0m
[stderr]: [31m│[0m   [2m2467 [0m[2m│   │   │   │   [0mmicrobatch_loss.mul_(microbatch_num_s [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m2468 [2m│   │   │   │   [0mmicrobatch_loss.backward(create_graph [31m│[0m
[stderr]: [31m│[0m   [2m2469 [0m[2m│   │   │   [0m                                          [31m│[0m
[stderr]: [31m│[0m   [2m2470 [0m[2m│   │   │   [0m[96mself[0m.engine.run_event(Event.AFTER_BACKWAR [31m│[0m
[stderr]: [31m│[0m   [2m2471 [0m                                                      [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/core/[0m[1;33mengine[0m [31m│[0m
[stderr]: [31m│[0m [1;33m.py[0m:[94m123[0m in [92msigterm_handler[0m                                   [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m120 [0m[2m# functions still run. Composer CLI launcher will give[0m [31m│[0m
[stderr]: [31m│[0m   [2m121 [0m[2m# SIGKILL.[0m                                             [31m│[0m
[stderr]: [31m│[0m   [2m122 [0m[94mdef[0m [92msigterm_handler[0m(signal, frame):                    [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m123 [2m│   [0msys.exit([94m128[0m + signal)                             [31m│[0m
[stderr]: [31m│[0m   [2m124 [0m                                                       [31m│[0m
[stderr]: [31m│[0m   [2m125 [0m                                                       [31m│[0m
[stderr]: [31m│[0m   [2m126 [0msignal.signal(signal.SIGTERM, sigterm_handler)         [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/[0m[1;33mexit_h[0m [31m│[0m
[stderr]: [31m│[0m [1;33mooks.py[0m:[94m36[0m in [92mexit[0m                                           [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m33 [0m[2m│   │   [0mcode = code [94mif[0m code [95mis[0m [95mnot[0m [94mNone[0m [94melse[0m [94m0[0m          [31m│[0m
[stderr]: [31m│[0m   [2m34 [0m[2m│   │   [0mcode = code [94mif[0m [96misinstance[0m(code, [96mint[0m) [94melse[0m [94m1[0m     [31m│[0m
[stderr]: [31m│[0m   [2m35 [0m[2m│   │   [0m[96mself[0m.exit_code = code                           [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m36 [2m│   │   [0m[96mself[0m._orig_exit(orig_code)  [2m# type: ignore[0m      [31m│[0m
[stderr]: [31m│[0m   [2m37 [0m[2m│   [0m                                                    [31m│[0m
[stderr]: [31m│[0m   [2m38 [0m[2m│   [0m[94mdef[0m [92mwas_ctrl_c[0m([96mself[0m) -> [96mbool[0m:                       [31m│[0m
[stderr]: [31m│[0m   [2m39 [0m[2m│   │   [0m[94mreturn[0m [96misinstance[0m([96mself[0m.exception, [96mKeyboardInter[0m [31m│[0m
[stderr]: [31m╰──────────────────────────────────────────────────────────────╯[0m
[stderr]: [1;91mSystemExit: [0m[1;36m143[0m

[stderr]: [3mDuring handling of the above exception, another exception [0m
[stderr]: [3moccurred:[0m

[stderr]: [31m╭─[0m[31m────────────[0m[31m [0m[1;31mTraceback [0m[1;2;31m(most recent call last)[0m[31m [0m[31m─────────────[0m[31m─╮[0m
[stderr]: [31m│[0m [2;33m/home/shuyaoli/LLM-Shearing/llmshearing/[0m[1;33mtrain.py[0m:[94m343[0m in      [31m│[0m
[stderr]: [31m│[0m [92m<module>[0m                                                     [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m340 [0m[2m│   [0mos.makedirs(save_dir, exist_ok=[94mTrue[0m)               [31m│[0m
[stderr]: [31m│[0m   [2m341 [0m[2m│   [0mtorch.save(cfg, save_dir + [33m"[0m[33m/config.pt[0m[33m"[0m)           [31m│[0m
[stderr]: [31m│[0m   [2m342 [0m[2m│   [0m                                                   [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m343 [2m│   [0mmain(cfg)                                          [31m│[0m
[stderr]: [31m│[0m   [2m344 [0m                                                       [31m│[0m
[stderr]: [31m│[0m   [2m345 [0m                                                       [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/home/shuyaoli/LLM-Shearing/llmshearing/[0m[1;33mtrain.py[0m:[94m327[0m in [92mmain[0m [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m324 [0m[2m│   │   [0mtrainer.eval()                                 [31m│[0m
[stderr]: [31m│[0m   [2m325 [0m[2m│   [0m                                                   [31m│[0m
[stderr]: [31m│[0m   [2m326 [0m[2m│   [0m[96mprint[0m([33m'[0m[33mStarting training...[0m[33m'[0m)                      [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m327 [2m│   [0mtrainer.fit()                                      [31m│[0m
[stderr]: [31m│[0m   [2m328 [0m[2m│   [0m                                                   [31m│[0m
[stderr]: [31m│[0m   [2m329 [0m[2m│   [0m[96mprint[0m([33m'[0m[33mDone.[0m[33m'[0m)                                     [31m│[0m
[stderr]: [31m│[0m   [2m330 [0m                                                       [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/trainer/[0m[1;33mtra[0m [31m│[0m
[stderr]: [31m│[0m [1;33miner.py[0m:[94m1876[0m in [92mfit[0m                                          [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m1873 [0m[2m│   │   │   [0m[96mself[0m.state.scaler = ClosureGradScaler() [94mi[0m [31m│[0m
[stderr]: [31m│[0m   [2m1874 [0m[2m│   │   [0m                                              [31m│[0m
[stderr]: [31m│[0m   [2m1875 [0m[2m│   │   [0m[96mself[0m.first_batch_complete = [94mFalse[0m             [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m1876 [2m│   │   [0m[96mself[0m._train_loop()                            [31m│[0m
[stderr]: [31m│[0m   [2m1877 [0m[2m│   [0m                                                  [31m│[0m
[stderr]: [31m│[0m   [2m1878 [0m[2m│   [0m[94mdef[0m [92mclose[0m([96mself[0m):                                  [31m│[0m
[stderr]: [31m│[0m   [2m1879 [0m[2;90m│   │   [0m[33m"""Shutdown the trainer.[0m                      [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/trainer/[0m[1;33mtra[0m [31m│[0m
[stderr]: [31m│[0m [1;33miner.py[0m:[94m2051[0m in [92m_train_loop[0m                                  [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m2048 [0m[2m│   │   │   │   │   │   [0m[96mself[0m.logger.log_metrics({[33m'[0m[33mtim[0m [31m│[0m
[stderr]: [31m│[0m   [2m2049 [0m[2m│   │   │   │   │   │   [0m[96mself[0m.logger.log_metrics({[33m'[0m[33mtim[0m [31m│[0m
[stderr]: [31m│[0m   [2m2050 [0m[2m│   │   │   │   │   [0m                                  [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m2051 [2m│   │   │   │   │   [0mtotal_loss_dict = [96mself[0m._train_bat [31m│[0m
[stderr]: [31m│[0m   [2m2052 [0m[2m│   │   │   │   │   [0m                                  [31m│[0m
[stderr]: [31m│[0m   [2m2053 [0m[2m│   │   │   │   │   [0m[94mif[0m use_grad_scaling:              [31m│[0m
[stderr]: [31m│[0m   [2m2054 [0m[2m│   │   │   │   │   │   [0m[96mself[0m.state.scaler.update()    [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/trainer/[0m[1;33mtra[0m [31m│[0m
[stderr]: [31m│[0m [1;33miner.py[0m:[94m2236[0m in [92m_train_batch[0m                                 [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m2233 [0m[2m│   │   │   │   │   │   │   │   │   │   │   │      [0mcl [31m│[0m
[stderr]: [31m│[0m   [2m2234 [0m[2m│   │   │   │   │   │   │   │   │   │   │   │      [0m_t [31m│[0m
[stderr]: [31m│[0m   [2m2235 [0m[2m│   │   │   │   │   │   [0m[94melse[0m:                         [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m2236 [2m│   │   │   │   │   │   │   [0moptimizer.step(closure=[94mla[0m [31m│[0m
[stderr]: [31m│[0m   [2m2237 [0m[2m│   │   │   │   │   │   │   │   [0mmicrobatches, loss_di [31m│[0m
[stderr]: [31m│[0m   [2m2238 [0m[2m│   │   │   │   [0m[94melse[0m:                                 [31m│[0m
[stderr]: [31m│[0m   [2m2239 [0m[2m│   │   │   │   │   [0m[96mself[0m._train_microbatches(microbat [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/torch/optim/[0m[1;33mlr_sched[0m [31m│[0m
[stderr]: [31m│[0m [1;33muler.py[0m:[94m69[0m in [92mwrapper[0m                                        [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m  66 [0m[2m│   │   │   │   [0minstance = instance_ref()             [31m│[0m
[stderr]: [31m│[0m   [2m  67 [0m[2m│   │   │   │   [0minstance._step_count += [94m1[0m             [31m│[0m
[stderr]: [31m│[0m   [2m  68 [0m[2m│   │   │   │   [0mwrapped = func.[92m__get__[0m(instance, [96mcls[0m) [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m  69 [2m│   │   │   │   [0m[94mreturn[0m wrapped(*args, **kwargs)       [31m│[0m
[stderr]: [31m│[0m   [2m  70 [0m[2m│   │   │   [0m                                          [31m│[0m
[stderr]: [31m│[0m   [2m  71 [0m[2m│   │   │   [0m[2m# Note that the returned function here is[0m [31m│[0m
[stderr]: [31m│[0m   [2m  72 [0m[2m│   │   │   [0m[2m# so attributes like `__func__` and `__se[0m [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/torch/optim/[0m[1;33moptimize[0m [31m│[0m
[stderr]: [31m│[0m [1;33mr.py[0m:[94m280[0m in [92mwrapper[0m                                          [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m277 [0m[2m│   │   │   │   │   │   │   [0m[94mraise[0m [96mRuntimeError[0m([33mf[0m[33m"[0m[33m{[0mfunc [31m│[0m
[stderr]: [31m│[0m   [2m278 [0m[2m│   │   │   │   │   │   │   │   │   │   │      [0m[33mf[0m[33m"[0m[33mbut g[0m [31m│[0m
[stderr]: [31m│[0m   [2m279 [0m[2m│   │   │   │   [0m                                       [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m280 [2m│   │   │   │   [0mout = func(*args, **kwargs)            [31m│[0m
[stderr]: [31m│[0m   [2m281 [0m[2m│   │   │   │   [0m[96mself[0m._optimizer_step_code()            [31m│[0m
[stderr]: [31m│[0m   [2m282 [0m[2m│   │   │   │   [0m                                       [31m│[0m
[stderr]: [31m│[0m   [2m283 [0m[2m│   │   │   │   [0m[2m# call optimizer step post hooks[0m       [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/torch/utils/[0m[1;33m_context[0m [31m│[0m
[stderr]: [31m│[0m [1;33mlib.py[0m:[94m115[0m in [92mdecorate_context[0m                               [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m112 [0m[2m│   [0m[1;95m@functools[0m.wraps(func)                             [31m│[0m
[stderr]: [31m│[0m   [2m113 [0m[2m│   [0m[94mdef[0m [92mdecorate_context[0m(*args, **kwargs):             [31m│[0m
[stderr]: [31m│[0m   [2m114 [0m[2m│   │   [0m[94mwith[0m ctx_factory():                            [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m115 [2m│   │   │   [0m[94mreturn[0m func(*args, **kwargs)               [31m│[0m
[stderr]: [31m│[0m   [2m116 [0m[2m│   [0m                                                   [31m│[0m
[stderr]: [31m│[0m   [2m117 [0m[2m│   [0m[94mreturn[0m decorate_context                            [31m│[0m
[stderr]: [31m│[0m   [2m118 [0m                                                       [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/optim/[0m[1;33mdecou[0m [31m│[0m
[stderr]: [31m│[0m [1;33mpled_weight_decay.py[0m:[94m276[0m in [92mstep[0m                             [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m273 [0m[2m│   │   [0mloss = [94mNone[0m                                    [31m│[0m
[stderr]: [31m│[0m   [2m274 [0m[2m│   │   [0m[94mif[0m closure [95mis[0m [95mnot[0m [94mNone[0m:                        [31m│[0m
[stderr]: [31m│[0m   [2m275 [0m[2m│   │   │   [0m[94mwith[0m torch.enable_grad():                  [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m276 [2m│   │   │   │   [0mloss = closure()                       [31m│[0m
[stderr]: [31m│[0m   [2m277 [0m[2m│   │   [0m                                               [31m│[0m
[stderr]: [31m│[0m   [2m278 [0m[2m│   │   [0m[94mfor[0m group [95min[0m [96mself[0m.param_groups:                [31m│[0m
[stderr]: [31m│[0m   [2m279 [0m[2m│   │   │   [0mparams_with_grad = []                      [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/trainer/[0m[1;33mtra[0m [31m│[0m
[stderr]: [31m│[0m [1;33miner.py[0m:[94m2236[0m in [92m<lambda>[0m                                     [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m2233 [0m[2m│   │   │   │   │   │   │   │   │   │   │   │      [0mcl [31m│[0m
[stderr]: [31m│[0m   [2m2234 [0m[2m│   │   │   │   │   │   │   │   │   │   │   │      [0m_t [31m│[0m
[stderr]: [31m│[0m   [2m2235 [0m[2m│   │   │   │   │   │   [0m[94melse[0m:                         [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m2236 [2m│   │   │   │   │   │   │   [0moptimizer.step(closure=[94mla[0m [31m│[0m
[stderr]: [31m│[0m   [2m2237 [0m[2m│   │   │   │   │   │   │   │   [0mmicrobatches, loss_di [31m│[0m
[stderr]: [31m│[0m   [2m2238 [0m[2m│   │   │   │   [0m[94melse[0m:                                 [31m│[0m
[stderr]: [31m│[0m   [2m2239 [0m[2m│   │   │   │   │   [0m[96mself[0m._train_microbatches(microbat [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/composer/trainer/[0m[1;33mtra[0m [31m│[0m
[stderr]: [31m│[0m [1;33miner.py[0m:[94m2339[0m in [92m_train_microbatches[0m                          [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m2336 [0m[2m│   │   │   [0m                                          [31m│[0m
[stderr]: [31m│[0m   [2m2337 [0m[2m│   │   │   [0m[94mfor[0m microbatch_idx, [96mself[0m.state.batch [95min[0m [96me[0m [31m│[0m
[stderr]: [31m│[0m   [2m2338 [0m[2m│   │   │   │   [0mis_final_microbatch = microbatch_idx  [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m2339 [2m│   │   │   │   [0mmicrobatch_loss_dict = [96mself[0m._train_mi [31m│[0m
[stderr]: [31m│[0m   [2m2340 [0m[2m│   │   │   │   [0m                                      [31m│[0m
[stderr]: [31m│[0m   [2m2341 [0m[2m│   │   │   │   [0m[2m# Aggregate each loss in microbatch_l[0m [31m│[0m
[stderr]: [31m│[0m   [2m2342 [0m[2m│   │   │   │   [0m[94mfor[0m k, microbatch_loss [95min[0m microbatch_ [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m [2;33m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_ut[0m [31m│[0m
[stderr]: [31m│[0m [2;33mils/[0m[1;33msignal_handling.py[0m:[94m66[0m in [92mhandler[0m                         [31m│[0m
[stderr]: [31m│[0m                                                              [31m│[0m
[stderr]: [31m│[0m   [2m63 [0m[2m│   [0m[94mdef[0m [92mhandler[0m(signum, frame):                         [31m│[0m
[stderr]: [31m│[0m   [2m64 [0m[2m│   │   [0m[2m# This following call uses `waitid` with WNOHAN[0m [31m│[0m
[stderr]: [31m│[0m   [2m65 [0m[2m│   │   [0m[2m# Python can still get and update the process s[0m [31m│[0m
[stderr]: [31m│[0m [31m❱ [0m66 [2m│   │   [0m_error_if_any_worker_fails()                    [31m│[0m
[stderr]: [31m│[0m   [2m67 [0m[2m│   │   [0m[94mif[0m previous_handler [95mis[0m [95mnot[0m [94mNone[0m:                [31m│[0m
[stderr]: [31m│[0m   [2m68 [0m[2m│   │   │   [0m[94massert[0m [96mcallable[0m(previous_handler)           [31m│[0m
[stderr]: [31m│[0m   [2m69 [0m[2m│   │   │   [0mprevious_handler(signum, frame)             [31m│[0m
[stderr]: [31m╰──────────────────────────────────────────────────────────────╯[0m
[stderr]: [1;91mRuntimeError: [0mDataLoader worker [1m([0mpid [1;36m10583[0m[1m)[0m exited unexpectedly 
[stderr]: with exit code [1;36m130[0m. Details are lost due to multiprocessing. 
[stderr]: Rerunning with [33mnum_workers[0m=[1;36m0[0m may give better error trace.
